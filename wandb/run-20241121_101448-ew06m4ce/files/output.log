  0%|                                                                          | 0/1500 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  1%|▍                                                                | 10/1500 [00:19<39:41,  1.60s/it]Traceback (most recent call last):
  File "/home/pooja/shashwat/LLM-Project/task3_lora.py", line 140, in <module>0 [01:19<01:36,  1.42it/s]
    trainer.train()
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/transformers/trainer.py", line 2123, in train
    return inner_training_loop(
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/transformers/trainer.py", line 2548, in _inner_training_loop
    self._maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/transformers/trainer.py", line 3004, in _maybe_log_save_evaluate
    metrics = self._evaluate(trial, ignore_keys_for_eval)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/transformers/trainer.py", line 2958, in _evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/transformers/trainer.py", line 3975, in evaluate
    output = eval_loop(
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/transformers/trainer.py", line 4169, in evaluation_loop
    losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/transformers/trainer.py", line 4385, in prediction_step
    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)
  File "/home/pooja/shashwat/LLM-Project/task3_lora.py", line 117, in compute_loss
    outputs = model(**inputs)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/accelerate/utils/operations.py", line 823, in forward
    return model_forward(*args, **kwargs)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/accelerate/utils/operations.py", line 811, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 14, in decorate_autocast
    return func(*args, **kwargs)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/peft/peft_model.py", line 732, in forward
    return self.base_model(
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/peft/tuners/tuners_utils.py", line 94, in forward
    return self.model.forward(*args, **kwargs)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/transformers/models/bloom/modeling_bloom.py", line 1102, in forward
    transformer_outputs = self.transformer(
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/transformers/models/bloom/modeling_bloom.py", line 693, in forward
    outputs = block(
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/transformers/models/bloom/modeling_bloom.py", line 426, in forward
    output = self.mlp(layernorm_output, residual)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/transformers/models/bloom/modeling_bloom.py", line 356, in forward
    intermediate_output = self.dense_4h_to_h(hidden_states)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/nn/modules.py", line 1009, in forward
    out = bnb.matmul(x, self.weight, bias=self.bias, state=self.state)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py", line 556, in matmul
    return MatMul8bitLt.apply(A, B, out, bias, state)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py", line 367, in forward
    outlier_idx = torch.unique(coo_tensorA.colidx)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/torch/_jit_internal.py", line 484, in fn
    return if_false(*args, **kwargs)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/torch/_jit_internal.py", line 484, in fn
    return if_false(*args, **kwargs)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/torch/functional.py", line 885, in _return_output
    output, _, _ = _unique_impl(input, sorted, return_inverse, return_counts, dim)
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/torch/functional.py", line 799, in _unique_impl
    output, inverse_indices, counts = torch._unique2(
KeyboardInterrupt
