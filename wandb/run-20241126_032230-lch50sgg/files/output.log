  0%|                                                                                                      | 0/6590 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  2%|█▎                                                                                        | 100/6590 [01:56<1:21:06,  1.33it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 11.3244, 'grad_norm': 213.91146850585938, 'learning_rate': 4.9688922610015176e-05, 'epoch': 0.08}
{'loss': 2.2359, 'grad_norm': 147.71636962890625, 'learning_rate': 4.9309559939301975e-05, 'epoch': 0.15}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 1.156957983970642, 'eval_accuracy': 0.7397572078907435, 'eval_runtime': 40.5612, 'eval_samples_per_second': 32.494, 'eval_steps_per_second': 4.068, 'epoch': 0.15}
  3%|██▋                                                                                       | 200/6590 [03:44<1:19:25,  1.34it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.8399, 'grad_norm': 119.8763198852539, 'learning_rate': 4.8930197268588774e-05, 'epoch': 0.23}
{'loss': 0.5377, 'grad_norm': 141.57186889648438, 'learning_rate': 4.855083459787557e-05, 'epoch': 0.3}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5552945733070374, 'eval_accuracy': 0.8710166919575114, 'eval_runtime': 35.6461, 'eval_samples_per_second': 36.975, 'eval_steps_per_second': 4.629, 'epoch': 0.3}
  5%|████                                                                                      | 300/6590 [05:33<1:17:53,  1.35it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.7217, 'grad_norm': 133.54437255859375, 'learning_rate': 4.817147192716237e-05, 'epoch': 0.38}
{'loss': 0.5658, 'grad_norm': 79.19104766845703, 'learning_rate': 4.779210925644917e-05, 'epoch': 0.46}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.46443161368370056, 'eval_accuracy': 0.9051593323216995, 'eval_runtime': 36.8786, 'eval_samples_per_second': 35.739, 'eval_steps_per_second': 4.474, 'epoch': 0.46}
  6%|█████▍                                                                                    | 400/6590 [07:26<1:17:30,  1.33it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.432, 'grad_norm': 131.61050415039062, 'learning_rate': 4.741274658573597e-05, 'epoch': 0.53}
{'loss': 0.5028, 'grad_norm': 3.5406391620635986, 'learning_rate': 4.703338391502277e-05, 'epoch': 0.61}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.4303937256336212, 'eval_accuracy': 0.9195751138088012, 'eval_runtime': 40.71, 'eval_samples_per_second': 32.375, 'eval_steps_per_second': 4.053, 'epoch': 0.61}
  8%|██████▊                                                                                   | 500/6590 [09:16<1:14:32,  1.36it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.4316, 'grad_norm': 94.57386016845703, 'learning_rate': 4.6654021244309566e-05, 'epoch': 0.68}
{'loss': 0.3545, 'grad_norm': 49.24083709716797, 'learning_rate': 4.627465857359636e-05, 'epoch': 0.76}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5590019822120667, 'eval_accuracy': 0.9066767830045523, 'eval_runtime': 39.8926, 'eval_samples_per_second': 33.039, 'eval_steps_per_second': 4.136, 'epoch': 0.76}
  9%|████████▏                                                                                 | 600/6590 [11:08<1:02:58,  1.59it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.4942, 'grad_norm': 22.62636947631836, 'learning_rate': 4.5895295902883156e-05, 'epoch': 0.83}
{'loss': 0.4122, 'grad_norm': 32.72343826293945, 'learning_rate': 4.5515933232169955e-05, 'epoch': 0.91}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.33087727427482605, 'eval_accuracy': 0.928679817905918, 'eval_runtime': 40.3511, 'eval_samples_per_second': 32.663, 'eval_steps_per_second': 4.089, 'epoch': 0.91}
 11%|█████████▌                                                                                | 700/6590 [12:57<1:14:02,  1.33it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.3749, 'grad_norm': 6.188870906829834, 'learning_rate': 4.5136570561456753e-05, 'epoch': 0.99}
{'loss': 0.4197, 'grad_norm': 1.7485771179199219, 'learning_rate': 4.475720789074355e-05, 'epoch': 1.06}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.4086495339870453, 'eval_accuracy': 0.9112291350531108, 'eval_runtime': 38.0538, 'eval_samples_per_second': 34.635, 'eval_steps_per_second': 4.336, 'epoch': 1.06}
 12%|██████████▉                                                                               | 800/6590 [14:48<1:12:20,  1.33it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.2434, 'grad_norm': 0.02595445141196251, 'learning_rate': 4.437784522003035e-05, 'epoch': 1.14}
{'loss': 0.28, 'grad_norm': 7.450055122375488, 'learning_rate': 4.399848254931715e-05, 'epoch': 1.21}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.45081982016563416, 'eval_accuracy': 0.9339908952959028, 'eval_runtime': 37.6134, 'eval_samples_per_second': 35.041, 'eval_steps_per_second': 4.387, 'epoch': 1.21}
 14%|████████████▎                                                                             | 900/6590 [16:35<1:11:19,  1.33it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.4631, 'grad_norm': 155.98582458496094, 'learning_rate': 4.361911987860395e-05, 'epoch': 1.29}
{'loss': 0.3052, 'grad_norm': 127.22373962402344, 'learning_rate': 4.323975720789075e-05, 'epoch': 1.37}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.41552263498306274, 'eval_accuracy': 0.9279210925644916, 'eval_runtime': 33.1701, 'eval_samples_per_second': 39.735, 'eval_steps_per_second': 4.974, 'epoch': 1.37}
 15%|█████████████▌                                                                           | 1000/6590 [18:28<1:08:35,  1.36it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.3155, 'grad_norm': 75.30511474609375, 'learning_rate': 4.2860394537177545e-05, 'epoch': 1.44}
{'loss': 0.3207, 'grad_norm': 86.03653717041016, 'learning_rate': 4.2481031866464344e-05, 'epoch': 1.52}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.3252263069152832, 'eval_accuracy': 0.9317147192716236, 'eval_runtime': 40.5115, 'eval_samples_per_second': 32.534, 'eval_steps_per_second': 4.073, 'epoch': 1.52}
 17%|██████████████▊                                                                          | 1100/6590 [20:17<1:08:01,  1.35it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.236, 'grad_norm': 43.35296630859375, 'learning_rate': 4.210166919575114e-05, 'epoch': 1.59}
{'loss': 0.2788, 'grad_norm': 0.7332712411880493, 'learning_rate': 4.172230652503794e-05, 'epoch': 1.67}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.3234710097312927, 'eval_accuracy': 0.9309559939301972, 'eval_runtime': 37.0251, 'eval_samples_per_second': 35.597, 'eval_steps_per_second': 4.456, 'epoch': 1.67}
 18%|████████████████▏                                                                        | 1200/6590 [22:06<1:06:47,  1.34it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.1969, 'grad_norm': 0.3316532075405121, 'learning_rate': 4.134294385432473e-05, 'epoch': 1.75}
{'loss': 0.2976, 'grad_norm': 0.28175726532936096, 'learning_rate': 4.096358118361153e-05, 'epoch': 1.82}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.3930612802505493, 'eval_accuracy': 0.9347496206373292, 'eval_runtime': 36.984, 'eval_samples_per_second': 35.637, 'eval_steps_per_second': 4.461, 'epoch': 1.82}
 20%|█████████████████▉                                                                         | 1300/6590 [23:59<59:19,  1.49it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.2842, 'grad_norm': 0.2693462371826172, 'learning_rate': 4.058421851289833e-05, 'epoch': 1.9}
{'loss': 0.1971, 'grad_norm': 0.7678662538528442, 'learning_rate': 4.020485584218513e-05, 'epoch': 1.97}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.43532171845436096, 'eval_accuracy': 0.9332321699544764, 'eval_runtime': 40.409, 'eval_samples_per_second': 32.617, 'eval_steps_per_second': 4.083, 'epoch': 1.97}
 21%|██████████████████▉                                                                      | 1400/6590 [25:49<1:04:35,  1.34it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.2561, 'grad_norm': 29.40587615966797, 'learning_rate': 3.982549317147193e-05, 'epoch': 2.05}
{'loss': 0.1988, 'grad_norm': 51.28316116333008, 'learning_rate': 3.9446130500758726e-05, 'epoch': 2.12}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.345506876707077, 'eval_accuracy': 0.9347496206373292, 'eval_runtime': 40.5091, 'eval_samples_per_second': 32.536, 'eval_steps_per_second': 4.073, 'epoch': 2.12}
 23%|████████████████████▎                                                                    | 1500/6590 [27:38<1:03:32,  1.34it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.2906, 'grad_norm': 6.1312479972839355, 'learning_rate': 3.9066767830045525e-05, 'epoch': 2.2}
{'loss': 0.1834, 'grad_norm': 0.015111790038645267, 'learning_rate': 3.8687405159332324e-05, 'epoch': 2.28}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5016640424728394, 'eval_accuracy': 0.9377845220030349, 'eval_runtime': 36.8798, 'eval_samples_per_second': 35.738, 'eval_steps_per_second': 4.474, 'epoch': 2.28}
 24%|█████████████████████▌                                                                   | 1600/6590 [29:28<1:03:21,  1.31it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.2552, 'grad_norm': 0.35899513959884644, 'learning_rate': 3.830804248861912e-05, 'epoch': 2.35}
{'loss': 0.3172, 'grad_norm': 44.90544509887695, 'learning_rate': 3.792867981790592e-05, 'epoch': 2.43}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.34031882882118225, 'eval_accuracy': 0.9294385432473444, 'eval_runtime': 37.841, 'eval_samples_per_second': 34.83, 'eval_steps_per_second': 4.36, 'epoch': 2.43}
 26%|██████████████████████▉                                                                  | 1700/6590 [31:17<1:00:18,  1.35it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.2484, 'grad_norm': 0.049495916813611984, 'learning_rate': 3.754931714719272e-05, 'epoch': 2.5}
{'loss': 0.1728, 'grad_norm': 0.0037676002830266953, 'learning_rate': 3.716995447647952e-05, 'epoch': 2.58}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.41024109721183777, 'eval_accuracy': 0.9355083459787557, 'eval_runtime': 37.3159, 'eval_samples_per_second': 35.32, 'eval_steps_per_second': 4.422, 'epoch': 2.58}
 27%|████████████████████████▎                                                                | 1800/6590 [33:07<1:00:00,  1.33it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.1617, 'grad_norm': 0.5523898005485535, 'learning_rate': 3.679059180576632e-05, 'epoch': 2.66}
{'loss': 0.2229, 'grad_norm': 79.6285400390625, 'learning_rate': 3.641122913505311e-05, 'epoch': 2.73}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.35045456886291504, 'eval_accuracy': 0.9355083459787557, 'eval_runtime': 35.6958, 'eval_samples_per_second': 36.923, 'eval_steps_per_second': 4.622, 'epoch': 2.73}
 29%|██████████████████████████▏                                                                | 1900/6590 [34:57<37:23,  2.09it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.1719, 'grad_norm': 3.03607177734375, 'learning_rate': 3.603186646433991e-05, 'epoch': 2.81}
{'loss': 0.2782, 'grad_norm': 128.38473510742188, 'learning_rate': 3.5652503793626706e-05, 'epoch': 2.88}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.3722980320453644, 'eval_accuracy': 0.9347496206373292, 'eval_runtime': 38.6055, 'eval_samples_per_second': 34.14, 'eval_steps_per_second': 4.274, 'epoch': 2.88}
 30%|███████████████████████████▌                                                               | 2000/6590 [36:50<57:10,  1.34it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.1662, 'grad_norm': 24.55169677734375, 'learning_rate': 3.5273141122913505e-05, 'epoch': 2.96}
{'loss': 0.0939, 'grad_norm': 0.4081290066242218, 'learning_rate': 3.4893778452200304e-05, 'epoch': 3.03}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.39634910225868225, 'eval_accuracy': 0.9385432473444613, 'eval_runtime': 40.517, 'eval_samples_per_second': 32.53, 'eval_steps_per_second': 4.072, 'epoch': 3.03}
 32%|████████████████████████████▉                                                              | 2100/6590 [38:35<55:40,  1.34it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.1428, 'grad_norm': 4.067914962768555, 'learning_rate': 3.45144157814871e-05, 'epoch': 3.11}
{'loss': 0.1086, 'grad_norm': 0.032243259251117706, 'learning_rate': 3.41350531107739e-05, 'epoch': 3.19}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.39753133058547974, 'eval_accuracy': 0.93247344461305, 'eval_runtime': 37.0108, 'eval_samples_per_second': 35.611, 'eval_steps_per_second': 4.458, 'epoch': 3.19}
 33%|██████████████████████████████▍                                                            | 2200/6590 [40:29<54:55,  1.33it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.053, 'grad_norm': 107.61858367919922, 'learning_rate': 3.37556904400607e-05, 'epoch': 3.26}
{'loss': 0.253, 'grad_norm': 152.2203369140625, 'learning_rate': 3.33763277693475e-05, 'epoch': 3.34}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.49178290367126465, 'eval_accuracy': 0.9294385432473444, 'eval_runtime': 40.3992, 'eval_samples_per_second': 32.624, 'eval_steps_per_second': 4.084, 'epoch': 3.34}
 35%|███████████████████████████████▊                                                           | 2300/6590 [42:19<51:12,  1.40it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.1796, 'grad_norm': 0.0020149473566561937, 'learning_rate': 3.29969650986343e-05, 'epoch': 3.41}
{'loss': 0.1827, 'grad_norm': 112.70951080322266, 'learning_rate': 3.2617602427921095e-05, 'epoch': 3.49}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.4288088381290436, 'eval_accuracy': 0.9317147192716236, 'eval_runtime': 40.3564, 'eval_samples_per_second': 32.659, 'eval_steps_per_second': 4.089, 'epoch': 3.49}
 36%|█████████████████████████████████▏                                                         | 2400/6590 [44:08<52:00,  1.34it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.1954, 'grad_norm': 115.6743392944336, 'learning_rate': 3.2238239757207894e-05, 'epoch': 3.57}
{'loss': 0.2122, 'grad_norm': 0.002449864987283945, 'learning_rate': 3.185887708649469e-05, 'epoch': 3.64}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.42786112427711487, 'eval_accuracy': 0.936267071320182, 'eval_runtime': 37.5973, 'eval_samples_per_second': 35.056, 'eval_steps_per_second': 4.389, 'epoch': 3.64}
 38%|██████████████████████████████████▌                                                        | 2500/6590 [45:56<26:14,  2.60it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.1055, 'grad_norm': 1.2651857105083764e-05, 'learning_rate': 3.1479514415781485e-05, 'epoch': 3.72}
{'loss': 0.0954, 'grad_norm': 0.9951881766319275, 'learning_rate': 3.110015174506828e-05, 'epoch': 3.79}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.4921671152114868, 'eval_accuracy': 0.9294385432473444, 'eval_runtime': 36.4888, 'eval_samples_per_second': 36.121, 'eval_steps_per_second': 4.522, 'epoch': 3.79}
 39%|███████████████████████████████████▉                                                       | 2600/6590 [47:49<49:17,  1.35it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.152, 'grad_norm': 0.020901955664157867, 'learning_rate': 3.072078907435508e-05, 'epoch': 3.87}
{'loss': 0.093, 'grad_norm': 0.0001531874731881544, 'learning_rate': 3.034142640364188e-05, 'epoch': 3.95}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.4321635067462921, 'eval_accuracy': 0.9370257966616085, 'eval_runtime': 40.4211, 'eval_samples_per_second': 32.607, 'eval_steps_per_second': 4.082, 'epoch': 3.95}
 41%|█████████████████████████████████████▎                                                     | 2700/6590 [49:37<48:16,  1.34it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.133, 'grad_norm': 4.284666538238525, 'learning_rate': 2.996206373292868e-05, 'epoch': 4.02}
{'loss': 0.1373, 'grad_norm': 0.0029305960051715374, 'learning_rate': 2.9582701062215478e-05, 'epoch': 4.1}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.49712663888931274, 'eval_accuracy': 0.9385432473444613, 'eval_runtime': 36.0905, 'eval_samples_per_second': 36.519, 'eval_steps_per_second': 4.572, 'epoch': 4.1}
 42%|██████████████████████████████████████▋                                                    | 2800/6590 [51:27<46:55,  1.35it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.138, 'grad_norm': 0.02741684950888157, 'learning_rate': 2.9203338391502277e-05, 'epoch': 4.17}
{'loss': 0.1106, 'grad_norm': 0.21738669276237488, 'learning_rate': 2.8823975720789075e-05, 'epoch': 4.25}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.4536098539829254, 'eval_accuracy': 0.9301972685887708, 'eval_runtime': 36.867, 'eval_samples_per_second': 35.75, 'eval_steps_per_second': 4.476, 'epoch': 4.25}
 44%|████████████████████████████████████████                                                   | 2900/6590 [53:20<46:02,  1.34it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.1705, 'grad_norm': 0.24687132239341736, 'learning_rate': 2.8444613050075874e-05, 'epoch': 4.32}
{'loss': 0.033, 'grad_norm': 0.1381472796201706, 'learning_rate': 2.8065250379362673e-05, 'epoch': 4.4}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.4249860346317291, 'eval_accuracy': 0.9400606980273141, 'eval_runtime': 40.5159, 'eval_samples_per_second': 32.53, 'eval_steps_per_second': 4.072, 'epoch': 4.4}
 46%|█████████████████████████████████████████▍                                                 | 3000/6590 [55:09<44:31,  1.34it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0653, 'grad_norm': 56.684364318847656, 'learning_rate': 2.768588770864947e-05, 'epoch': 4.48}
{'loss': 0.0542, 'grad_norm': 0.5022388100624084, 'learning_rate': 2.730652503793627e-05, 'epoch': 4.55}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.44324156641960144, 'eval_accuracy': 0.9377845220030349, 'eval_runtime': 40.0048, 'eval_samples_per_second': 32.946, 'eval_steps_per_second': 4.125, 'epoch': 4.55}
 47%|██████████████████████████████████████████▊                                                | 3100/6590 [57:00<27:38,  2.10it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0379, 'grad_norm': 0.0076296147890388966, 'learning_rate': 2.692716236722307e-05, 'epoch': 4.63}
{'loss': 0.2063, 'grad_norm': 0.004196261055767536, 'learning_rate': 2.6547799696509867e-05, 'epoch': 4.7}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.44266751408576965, 'eval_accuracy': 0.9347496206373292, 'eval_runtime': 40.2716, 'eval_samples_per_second': 32.728, 'eval_steps_per_second': 4.097, 'epoch': 4.7}
 49%|████████████████████████████████████████████▏                                              | 3200/6590 [58:48<41:41,  1.35it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0773, 'grad_norm': 133.39871215820312, 'learning_rate': 2.616843702579666e-05, 'epoch': 4.78}
{'loss': 0.0599, 'grad_norm': 0.00014093948993831873, 'learning_rate': 2.5789074355083458e-05, 'epoch': 4.86}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5247172713279724, 'eval_accuracy': 0.936267071320182, 'eval_runtime': 37.7435, 'eval_samples_per_second': 34.92, 'eval_steps_per_second': 4.372, 'epoch': 4.86}
 50%|████████████████████████████████████████████▌                                            | 3300/6590 [1:00:38<40:47,  1.34it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.1164, 'grad_norm': 3.165871021337807e-06, 'learning_rate': 2.5409711684370256e-05, 'epoch': 4.93}
{'loss': 0.1194, 'grad_norm': 0.0008030773606151342, 'learning_rate': 2.5030349013657055e-05, 'epoch': 5.01}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.4832461178302765, 'eval_accuracy': 0.936267071320182, 'eval_runtime': 37.3795, 'eval_samples_per_second': 35.26, 'eval_steps_per_second': 4.414, 'epoch': 5.01}
 52%|█████████████████████████████████████████████▉                                           | 3400/6590 [1:02:25<39:23,  1.35it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.1285, 'grad_norm': 0.08415067940950394, 'learning_rate': 2.4650986342943854e-05, 'epoch': 5.08}
{'loss': 0.0363, 'grad_norm': 1.1714941263198853, 'learning_rate': 2.4271623672230652e-05, 'epoch': 5.16}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5290819406509399, 'eval_accuracy': 0.9339908952959028, 'eval_runtime': 32.5864, 'eval_samples_per_second': 40.446, 'eval_steps_per_second': 5.063, 'epoch': 5.16}
 53%|███████████████████████████████████████████████▎                                         | 3500/6590 [1:04:18<36:54,  1.40it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.01, 'grad_norm': 90.51428985595703, 'learning_rate': 2.389226100151745e-05, 'epoch': 5.24}
{'loss': 0.0828, 'grad_norm': 8.074463844299316, 'learning_rate': 2.351289833080425e-05, 'epoch': 5.31}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5017427802085876, 'eval_accuracy': 0.9347496206373292, 'eval_runtime': 40.3537, 'eval_samples_per_second': 32.661, 'eval_steps_per_second': 4.089, 'epoch': 5.31}
 55%|████████████████████████████████████████████████▌                                        | 3600/6590 [1:06:09<36:50,  1.35it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.1129, 'grad_norm': 5.500027656555176, 'learning_rate': 2.3133535660091048e-05, 'epoch': 5.39}
{'loss': 0.1248, 'grad_norm': 0.0005396208725869656, 'learning_rate': 2.2754172989377847e-05, 'epoch': 5.46}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.49706777930259705, 'eval_accuracy': 0.9332321699544764, 'eval_runtime': 38.087, 'eval_samples_per_second': 34.605, 'eval_steps_per_second': 4.332, 'epoch': 5.46}
 56%|█████████████████████████████████████████████████▉                                       | 3700/6590 [1:07:58<35:41,  1.35it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0194, 'grad_norm': 0.0013116287300363183, 'learning_rate': 2.2374810318664642e-05, 'epoch': 5.54}
{'loss': 0.0458, 'grad_norm': 9.97371807898162e-06, 'learning_rate': 2.199544764795144e-05, 'epoch': 5.61}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5016979575157166, 'eval_accuracy': 0.9332321699544764, 'eval_runtime': 37.1894, 'eval_samples_per_second': 35.44, 'eval_steps_per_second': 4.437, 'epoch': 5.61}
 58%|███████████████████████████████████████████████████▎                                     | 3800/6590 [1:09:51<33:38,  1.38it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0033, 'grad_norm': 0.001902220188640058, 'learning_rate': 2.161608497723824e-05, 'epoch': 5.69}
{'loss': 0.0891, 'grad_norm': 0.00011743989307433367, 'learning_rate': 2.1236722306525038e-05, 'epoch': 5.77}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5231649875640869, 'eval_accuracy': 0.936267071320182, 'eval_runtime': 40.0657, 'eval_samples_per_second': 32.896, 'eval_steps_per_second': 4.118, 'epoch': 5.77}
 59%|████████████████████████████████████████████████████▋                                    | 3900/6590 [1:11:39<33:09,  1.35it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.086, 'grad_norm': 0.0032543321140110493, 'learning_rate': 2.0857359635811837e-05, 'epoch': 5.84}
{'loss': 0.0376, 'grad_norm': 0.0861840546131134, 'learning_rate': 2.0477996965098635e-05, 'epoch': 5.92}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5400288105010986, 'eval_accuracy': 0.936267071320182, 'eval_runtime': 40.2505, 'eval_samples_per_second': 32.745, 'eval_steps_per_second': 4.099, 'epoch': 5.92}
 61%|██████████████████████████████████████████████████████                                   | 4000/6590 [1:13:27<32:18,  1.34it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0225, 'grad_norm': 10.595189094543457, 'learning_rate': 2.009863429438543e-05, 'epoch': 5.99}
{'loss': 0.0667, 'grad_norm': 0.002000432927161455, 'learning_rate': 1.971927162367223e-05, 'epoch': 6.07}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5242927074432373, 'eval_accuracy': 0.9309559939301972, 'eval_runtime': 35.5445, 'eval_samples_per_second': 37.08, 'eval_steps_per_second': 4.642, 'epoch': 6.07}
 62%|███████████████████████████████████████████████████████▎                                 | 4100/6590 [1:15:15<28:58,  1.43it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0594, 'grad_norm': 0.00043474798440001905, 'learning_rate': 1.9339908952959028e-05, 'epoch': 6.15}
{'loss': 0.0103, 'grad_norm': 1.5736544810351916e-05, 'learning_rate': 1.8960546282245827e-05, 'epoch': 6.22}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5658547282218933, 'eval_accuracy': 0.9309559939301972, 'eval_runtime': 37.1509, 'eval_samples_per_second': 35.477, 'eval_steps_per_second': 4.441, 'epoch': 6.22}
 64%|████████████████████████████████████████████████████████▋                                | 4200/6590 [1:17:19<32:51,  1.21it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0161, 'grad_norm': 0.00029220961732789874, 'learning_rate': 1.8581183611532625e-05, 'epoch': 6.3}
{'loss': 0.0296, 'grad_norm': 1.80595486654056e-06, 'learning_rate': 1.8201820940819424e-05, 'epoch': 6.37}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5777298212051392, 'eval_accuracy': 0.93247344461305, 'eval_runtime': 37.132, 'eval_samples_per_second': 35.495, 'eval_steps_per_second': 4.444, 'epoch': 6.37}
 65%|██████████████████████████████████████████████████████████                               | 4300/6590 [1:19:09<28:23,  1.34it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0529, 'grad_norm': 0.2113787978887558, 'learning_rate': 1.7822458270106223e-05, 'epoch': 6.45}
{'loss': 0.0749, 'grad_norm': 0.024771390482783318, 'learning_rate': 1.7443095599393018e-05, 'epoch': 6.53}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5332936644554138, 'eval_accuracy': 0.9317147192716236, 'eval_runtime': 35.3402, 'eval_samples_per_second': 37.295, 'eval_steps_per_second': 4.669, 'epoch': 6.53}
 67%|███████████████████████████████████████████████████████████▍                             | 4400/6590 [1:21:18<31:33,  1.16it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0135, 'grad_norm': 0.000350525660905987, 'learning_rate': 1.7063732928679817e-05, 'epoch': 6.6}
{'loss': 0.0619, 'grad_norm': 0.0018838047981262207, 'learning_rate': 1.6684370257966615e-05, 'epoch': 6.68}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5259581208229065, 'eval_accuracy': 0.9332321699544764, 'eval_runtime': 51.5239, 'eval_samples_per_second': 25.58, 'eval_steps_per_second': 3.202, 'epoch': 6.68}
 68%|████████████████████████████████████████████████████████████▊                            | 4500/6590 [1:23:25<31:21,  1.11it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0252, 'grad_norm': 0.29190823435783386, 'learning_rate': 1.6305007587253414e-05, 'epoch': 6.75}
{'loss': 0.0433, 'grad_norm': 22.63689422607422, 'learning_rate': 1.5925644916540213e-05, 'epoch': 6.83}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5270440578460693, 'eval_accuracy': 0.9256449165402124, 'eval_runtime': 53.8084, 'eval_samples_per_second': 24.494, 'eval_steps_per_second': 3.066, 'epoch': 6.83}
 70%|██████████████████████████████████████████████████████████████                           | 4600/6590 [1:25:16<19:21,  1.71it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0064, 'grad_norm': 0.003008110448718071, 'learning_rate': 1.554628224582701e-05, 'epoch': 6.9}
{'loss': 0.0773, 'grad_norm': 0.000987750245258212, 'learning_rate': 1.5166919575113808e-05, 'epoch': 6.98}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.555033266544342, 'eval_accuracy': 0.9355083459787557, 'eval_runtime': 36.6126, 'eval_samples_per_second': 35.999, 'eval_steps_per_second': 4.507, 'epoch': 6.98}
 71%|███████████████████████████████████████████████████████████████▍                         | 4700/6590 [1:27:08<23:35,  1.33it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0164, 'grad_norm': 0.00011821168300230056, 'learning_rate': 1.4787556904400607e-05, 'epoch': 7.06}
{'loss': 0.0017, 'grad_norm': 0.1346946507692337, 'learning_rate': 1.4408194233687405e-05, 'epoch': 7.13}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5872272849082947, 'eval_accuracy': 0.9347496206373292, 'eval_runtime': 40.4552, 'eval_samples_per_second': 32.579, 'eval_steps_per_second': 4.079, 'epoch': 7.13}
 73%|████████████████████████████████████████████████████████████████▊                        | 4800/6590 [1:28:55<22:21,  1.33it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0001, 'grad_norm': 0.04706873372197151, 'learning_rate': 1.4028831562974204e-05, 'epoch': 7.21}
{'loss': 0.0001, 'grad_norm': 2.4297601157741155e-06, 'learning_rate': 1.3649468892261003e-05, 'epoch': 7.28}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5569226741790771, 'eval_accuracy': 0.9332321699544764, 'eval_runtime': 37.1769, 'eval_samples_per_second': 35.452, 'eval_steps_per_second': 4.438, 'epoch': 7.28}
 74%|██████████████████████████████████████████████████████████████████▏                      | 4900/6590 [1:30:48<20:27,  1.38it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0002, 'grad_norm': 0.6680137515068054, 'learning_rate': 1.3270106221547801e-05, 'epoch': 7.36}
{'loss': 0.0768, 'grad_norm': 0.004569754470139742, 'learning_rate': 1.28907435508346e-05, 'epoch': 7.44}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.564407467842102, 'eval_accuracy': 0.9309559939301972, 'eval_runtime': 40.2572, 'eval_samples_per_second': 32.74, 'eval_steps_per_second': 4.099, 'epoch': 7.44}
 76%|███████████████████████████████████████████████████████████████████▌                     | 5000/6590 [1:32:37<12:34,  2.11it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0365, 'grad_norm': 0.012142150662839413, 'learning_rate': 1.2511380880121395e-05, 'epoch': 7.51}
{'loss': 0.0012, 'grad_norm': 25.695775985717773, 'learning_rate': 1.2132018209408196e-05, 'epoch': 7.59}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.603702962398529, 'eval_accuracy': 0.936267071320182, 'eval_runtime': 40.4663, 'eval_samples_per_second': 32.57, 'eval_steps_per_second': 4.077, 'epoch': 7.59}
 77%|████████████████████████████████████████████████████████████████████▉                    | 5100/6590 [1:34:27<18:21,  1.35it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0535, 'grad_norm': 0.0054298704490065575, 'learning_rate': 1.1752655538694993e-05, 'epoch': 7.66}
{'loss': 0.0, 'grad_norm': 0.008402778767049313, 'learning_rate': 1.1373292867981791e-05, 'epoch': 7.74}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.6042547821998596, 'eval_accuracy': 0.9317147192716236, 'eval_runtime': 37.6774, 'eval_samples_per_second': 34.981, 'eval_steps_per_second': 4.379, 'epoch': 7.74}
 79%|██████████████████████████████████████████████████████████████████████▏                  | 5200/6590 [1:36:14<11:57,  1.94it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.06, 'grad_norm': 0.00018511965754441917, 'learning_rate': 1.099393019726859e-05, 'epoch': 7.81}
{'loss': 0.0442, 'grad_norm': 3.0270678053057054e-07, 'learning_rate': 1.0614567526555387e-05, 'epoch': 7.89}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5561861991882324, 'eval_accuracy': 0.9339908952959028, 'eval_runtime': 34.2581, 'eval_samples_per_second': 38.473, 'eval_steps_per_second': 4.816, 'epoch': 7.89}
 80%|███████████████████████████████████████████████████████████████████████▌                 | 5300/6590 [1:38:07<16:00,  1.34it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0255, 'grad_norm': 0.0009965052595362067, 'learning_rate': 1.0235204855842186e-05, 'epoch': 7.97}
{'loss': 0.0231, 'grad_norm': 0.21551933884620667, 'learning_rate': 9.855842185128984e-06, 'epoch': 8.04}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5677806735038757, 'eval_accuracy': 0.9355083459787557, 'eval_runtime': 40.4645, 'eval_samples_per_second': 32.572, 'eval_steps_per_second': 4.078, 'epoch': 8.04}
 82%|████████████████████████████████████████████████████████████████████████▉                | 5400/6590 [1:39:55<14:47,  1.34it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0175, 'grad_norm': 45.837493896484375, 'learning_rate': 9.476479514415781e-06, 'epoch': 8.12}
{'loss': 0.0214, 'grad_norm': 0.0001412070996593684, 'learning_rate': 9.09711684370258e-06, 'epoch': 8.19}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.6519364714622498, 'eval_accuracy': 0.93247344461305, 'eval_runtime': 35.7415, 'eval_samples_per_second': 36.876, 'eval_steps_per_second': 4.616, 'epoch': 8.19}
 83%|██████████████████████████████████████████████████████████████████████████▎              | 5500/6590 [1:41:45<13:26,  1.35it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0001, 'grad_norm': 0.002411994617432356, 'learning_rate': 8.717754172989379e-06, 'epoch': 8.27}
{'loss': 0.0036, 'grad_norm': 0.030967997387051582, 'learning_rate': 8.338391502276175e-06, 'epoch': 8.35}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5869364142417908, 'eval_accuracy': 0.93247344461305, 'eval_runtime': 36.894, 'eval_samples_per_second': 35.724, 'eval_steps_per_second': 4.472, 'epoch': 8.35}
 85%|███████████████████████████████████████████████████████████████████████████▋             | 5600/6590 [1:43:39<12:09,  1.36it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0011, 'grad_norm': 0.0006498484872281551, 'learning_rate': 7.959028831562974e-06, 'epoch': 8.42}
{'loss': 0.0211, 'grad_norm': 1.0240271876682527e-05, 'learning_rate': 7.579666160849773e-06, 'epoch': 8.5}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.6052293181419373, 'eval_accuracy': 0.9339908952959028, 'eval_runtime': 40.5881, 'eval_samples_per_second': 32.473, 'eval_steps_per_second': 4.065, 'epoch': 8.5}
 86%|████████████████████████████████████████████████████████████████████████████▉            | 5700/6590 [1:45:29<11:06,  1.34it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0001, 'grad_norm': 0.08117543160915375, 'learning_rate': 7.2003034901365714e-06, 'epoch': 8.57}
{'loss': 0.0134, 'grad_norm': 0.035075485706329346, 'learning_rate': 6.820940819423368e-06, 'epoch': 8.65}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.6019893288612366, 'eval_accuracy': 0.9332321699544764, 'eval_runtime': 40.5463, 'eval_samples_per_second': 32.506, 'eval_steps_per_second': 4.069, 'epoch': 8.65}
 88%|██████████████████████████████████████████████████████████████████████████████▎          | 5800/6590 [1:47:20<06:17,  2.09it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0266, 'grad_norm': 0.009470334276556969, 'learning_rate': 6.441578148710167e-06, 'epoch': 8.73}
{'loss': 0.027, 'grad_norm': 0.006470908876508474, 'learning_rate': 6.062215477996965e-06, 'epoch': 8.8}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5922113656997681, 'eval_accuracy': 0.9339908952959028, 'eval_runtime': 39.1894, 'eval_samples_per_second': 33.632, 'eval_steps_per_second': 4.21, 'epoch': 8.8}
 90%|███████████████████████████████████████████████████████████████████████████████▋         | 5900/6590 [1:49:09<08:32,  1.35it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.1055, 'grad_norm': 0.013268755748867989, 'learning_rate': 5.6828528072837635e-06, 'epoch': 8.88}
{'loss': 0.0001, 'grad_norm': 0.010779811069369316, 'learning_rate': 5.303490136570561e-06, 'epoch': 8.95}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5830511450767517, 'eval_accuracy': 0.9301972685887708, 'eval_runtime': 37.82, 'eval_samples_per_second': 34.849, 'eval_steps_per_second': 4.363, 'epoch': 8.95}
 91%|█████████████████████████████████████████████████████████████████████████████████        | 6000/6590 [1:50:58<07:20,  1.34it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0099, 'grad_norm': 0.000636724173091352, 'learning_rate': 4.924127465857359e-06, 'epoch': 9.03}
{'loss': 0.0103, 'grad_norm': 4.826431177207269e-05, 'learning_rate': 4.544764795144158e-06, 'epoch': 9.1}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5648266077041626, 'eval_accuracy': 0.9309559939301972, 'eval_runtime': 37.4928, 'eval_samples_per_second': 35.153, 'eval_steps_per_second': 4.401, 'epoch': 9.1}
 93%|██████████████████████████████████████████████████████████████████████████████████▍      | 6100/6590 [1:52:49<06:05,  1.34it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0185, 'grad_norm': 1.1575692892074585, 'learning_rate': 4.1654021244309564e-06, 'epoch': 9.18}
{'loss': 0.0001, 'grad_norm': 0.0031530973501503468, 'learning_rate': 3.7860394537177547e-06, 'epoch': 9.26}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5795554518699646, 'eval_accuracy': 0.9355083459787557, 'eval_runtime': 32.9702, 'eval_samples_per_second': 39.975, 'eval_steps_per_second': 5.005, 'epoch': 9.26}
 94%|███████████████████████████████████████████████████████████████████████████████████▋     | 6200/6590 [1:54:42<04:36,  1.41it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0064, 'grad_norm': 0.017828132957220078, 'learning_rate': 3.4066767830045525e-06, 'epoch': 9.33}
{'loss': 0.04, 'grad_norm': 0.00037764458102174103, 'learning_rate': 3.034901365705615e-06, 'epoch': 9.41}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5850403308868408, 'eval_accuracy': 0.9347496206373292, 'eval_runtime': 40.3229, 'eval_samples_per_second': 32.686, 'eval_steps_per_second': 4.092, 'epoch': 9.41}
 96%|█████████████████████████████████████████████████████████████████████████████████████    | 6300/6590 [1:56:32<03:31,  1.37it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0019, 'grad_norm': 0.00017926610598806292, 'learning_rate': 2.655538694992413e-06, 'epoch': 9.48}
{'loss': 0.0003, 'grad_norm': 0.0066228643991053104, 'learning_rate': 2.276176024279211e-06, 'epoch': 9.56}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5859032273292542, 'eval_accuracy': 0.9332321699544764, 'eval_runtime': 39.2947, 'eval_samples_per_second': 33.541, 'eval_steps_per_second': 4.199, 'epoch': 9.56}
 97%|██████████████████████████████████████████████████████████████████████████████████████▍  | 6400/6590 [1:58:20<02:26,  1.30it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0, 'grad_norm': 0.0027421293780207634, 'learning_rate': 1.8968133535660093e-06, 'epoch': 9.64}
{'loss': 0.0115, 'grad_norm': 0.001115014893002808, 'learning_rate': 1.5174506828528075e-06, 'epoch': 9.71}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5825594663619995, 'eval_accuracy': 0.9339908952959028, 'eval_runtime': 37.3304, 'eval_samples_per_second': 35.306, 'eval_steps_per_second': 4.42, 'epoch': 9.71}
 99%|███████████████████████████████████████████████████████████████████████████████████████▊ | 6500/6590 [2:00:20<01:00,  1.48it/s]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 0.0672, 'grad_norm': 130.67298889160156, 'learning_rate': 1.1380880121396055e-06, 'epoch': 9.79}
{'loss': 0.002, 'grad_norm': 1.3097404917061795e-05, 'learning_rate': 7.587253414264037e-07, 'epoch': 9.86}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")                                 
{'eval_loss': 0.5839998126029968, 'eval_accuracy': 0.9332321699544764, 'eval_runtime': 40.4675, 'eval_samples_per_second': 32.569, 'eval_steps_per_second': 4.077, 'epoch': 9.86}
100%|█████████████████████████████████████████████████████████████████████████████████████████| 6590/6590 [2:01:21<00:00,  1.38it/s]Traceback (most recent call last):
{'loss': 0.0, 'grad_norm': 0.0001377203589072451, 'learning_rate': 3.793626707132019e-07, 'epoch': 9.94}
  File "/home/pooja/shashwat/LLM-Project/task3_guj_lora.py", line 173, in <module>
    trainer.train()
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/transformers/trainer.py", line 2123, in train
    return inner_training_loop(
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/transformers/trainer.py", line 2601, in _inner_training_loop
    self._load_best_model()
  File "/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/transformers/trainer.py", line 2891, in _load_best_model
    active_adapter = model.active_adapters[0]
TypeError: 'method' object is not subscriptable
