  0%|                                                                           | 0/126 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
 79%|███████████████████████████████████████████████████▌             | 100/126 [03:47<00:55,  2.13s/it]/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
{'loss': 5.0424, 'grad_norm': 48.893741607666016, 'learning_rate': 3.3730158730158734e-05, 'epoch': 0.79}
{'loss': 1.1677, 'grad_norm': 116.544677734375, 'learning_rate': 1.388888888888889e-05, 'epoch': 1.59}
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")     
{'eval_loss': 1.323258638381958, 'eval_accuracy': 0.565, 'eval_runtime': 10.6044, 'eval_samples_per_second': 18.86, 'eval_steps_per_second': 1.226, 'epoch': 1.59}
100%|█████████████████████████████████████████████████████████████████| 126/126 [04:42<00:00,  2.03s/it]2024-11-21 12:13:16,085 - WARNING - __main__ - Error while loading best model: 'method' object is not subscriptable
2024-11-21 12:13:16,087 - INFO - __main__ - Loaded best model and activated adapter: <bound method PeftAdapterMixin.active_adapters of BloomForSequenceClassification(
  (transformer): BloomModel(
    (word_embeddings): Embedding(250880, 2048)
    (word_embeddings_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
    (h): ModuleList(
      (0-23): 24 x BloomBlock(
        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (self_attention): BloomAttention(
          (query_key_value): Linear8bitLt(
            in_features=2048, out_features=6144, bias=True
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.1, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=2048, out_features=8, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=8, out_features=6144, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (dense): Linear8bitLt(
            in_features=2048, out_features=2048, bias=True
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.1, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): Linear(in_features=2048, out_features=8, bias=False)
            )
            (lora_B): ModuleDict(
              (default): Linear(in_features=8, out_features=2048, bias=False)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
          )
          (attention_dropout): Dropout(p=0.0, inplace=False)
        )
        (post_attention_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        (mlp): BloomMLP(
          (dense_h_to_4h): Linear8bitLt(in_features=2048, out_features=8192, bias=True)
          (gelu_impl): BloomGelu()
          (dense_4h_to_h): Linear8bitLt(in_features=8192, out_features=2048, bias=True)
        )
      )
    )
    (ln_f): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
  )
  (score): ModulesToSaveWrapper(
    (original_module): Linear(in_features=2048, out_features=2, bias=False)
    (modules_to_save): ModuleDict(
      (default): Linear(in_features=2048, out_features=2, bias=False)
    )
  )
)>
100%|█████████████████████████████████████████████████████████████████| 126/126 [04:42<00:00,  2.24s/it]
{'train_runtime': 283.8681, 'train_samples_per_second': 7.046, 'train_steps_per_second': 0.444, 'train_loss': 2.7115397528996543, 'epoch': 2.0}
Evaluation on English validation set:
/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
100%|███████████████████████████████████████████████████████████████████| 13/13 [00:09<00:00,  1.33it/s]
Accuracy on English validation set: 0.585
Evaluation on Bengali dataset:
/home/pooja/anaconda3/envs/kg_rag/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
100%|███████████████████████████████████████████████████████████████████| 13/13 [00:09<00:00,  1.33it/s]
Accuracy on Bengali test data: 0.535
Train-Val Loss Saved!!
Val Acc Saved!!
